{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0df1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c986d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-16d340229f74c1d5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/pj11/.cache/huggingface/datasets/csv/default-16d340229f74c1d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425f8d9faf804d3ea684102e0f7ff0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb4da3665c24ccdac26dc96c6fbf0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/pj11/.cache/huggingface/datasets/csv/default-16d340229f74c1d5/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c58ee1cf5f1436ebe1d67241b6a70ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files={'train':['esol_bert_ds1.csv', 'esol_bert_ds2.csv',\n",
    "                                                   'esol_bert_ds3.csv', 'esol_bert_ds4.csv'],\n",
    "                                          'validation':'esol_bert_ds5.csv',\n",
    "                                          'test': 'esol_bert_ds6.csv'}, delimiter=',', column_names =['smiles', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9874b270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': 'CC34CC(O)C1(F)C(CCC2=CC(=O)C=CC12C)C3CC(O)C4(O)C(=O)CO',\n",
       " 'label': -3.68}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9894c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepchem/ChemBERTa-77M-MTR\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"smiles\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b48772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd0a6d5cfba47c6874b6703d3df50f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0769ca7b41bc48b78c31d6f5d2d3d744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f29326b73d44f1a959487c19dac3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009f538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 764\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 190\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f040040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=random_state).select(range(1000))\n",
    "#small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=random_state).select(range(1000))\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"]\n",
    "small_test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b5f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepchem/ChemBERTa-77M-MTR were not used when initializing RobertaForSequenceClassification: ['norm_mean', 'regression.out_proj.weight', 'norm_std', 'regression.out_proj.bias', 'regression.dense.bias', 'regression.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at deepchem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# for regression, num_labels=1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"deepchem/ChemBERTa-77M-MTR\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90cab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_metric = evaluate.load(\"mae\")\n",
    "mse_metric = evaluate.load(\"mse\")\n",
    "pearsonr_metric = evaluate.load(\"pearsonr\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    #logits, labels = eval_pred\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics.update({'mae': mae_metric.compute(predictions=predictions, references=labels)})\n",
    "    metrics.update({'rmse': mse_metric.compute(predictions=predictions, references=labels, squared=False)})\n",
    "    metrics.update({'pearsonr': pearsonr_metric.compute(predictions=predictions, references=labels)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574f2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_output_dir = 'C:/Users/pj11/Documents/bert_finetune2/esol_fold1/'\n",
    "model_output_path = f'{para_output_dir}/model'\n",
    "\n",
    "training_args = TrainingArguments(output_dir=para_output_dir, \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  #per_device_train_batch_size = 64,\n",
    "                                  #per_device_eval_batch_size = 64,\n",
    "                                  num_train_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4510db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "794a8620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 764\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1200\n",
      "  Number of trainable parameters = 3427825\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 02:44, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Pearsonr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>12.984365</td>\n",
       "      <td>{'mae': 2.984160893881007}</td>\n",
       "      <td>{'mse': 3.613996773981356}</td>\n",
       "      <td>{'pearsonr': -0.11209459778270305}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.119222</td>\n",
       "      <td>{'mae': 2.3799418987020067}</td>\n",
       "      <td>{'mse': 3.0280281058379046}</td>\n",
       "      <td>{'pearsonr': -0.2699275622576053}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.927746</td>\n",
       "      <td>{'mae': 1.5420490744474686}</td>\n",
       "      <td>{'mse': 1.9815574817595984}</td>\n",
       "      <td>{'pearsonr': 0.5920640524214649}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.750324</td>\n",
       "      <td>{'mae': 1.03072284287528}</td>\n",
       "      <td>{'mse': 1.3195232950462024}</td>\n",
       "      <td>{'pearsonr': 0.8013015831669301}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.616080</td>\n",
       "      <td>{'mae': 1.0236645638060413}</td>\n",
       "      <td>{'mse': 1.2680229551192121}</td>\n",
       "      <td>{'pearsonr': 0.8326592459150237}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.389485</td>\n",
       "      <td>{'mae': 0.953807479025502}</td>\n",
       "      <td>{'mse': 1.1751459717091104}</td>\n",
       "      <td>{'pearsonr': 0.8607159680807849}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.265544</td>\n",
       "      <td>{'mae': 0.9003752521484306}</td>\n",
       "      <td>{'mse': 1.1235506921233867}</td>\n",
       "      <td>{'pearsonr': 0.8679903853346769}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.093065</td>\n",
       "      <td>{'mae': 0.8382178410936735}</td>\n",
       "      <td>{'mse': 1.0430409879614053}</td>\n",
       "      <td>{'pearsonr': 0.886021047608962}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.122080</td>\n",
       "      <td>{'mae': 0.8536650743727622}</td>\n",
       "      <td>{'mse': 1.0581573086346625}</td>\n",
       "      <td>{'pearsonr': 0.8880543567678005}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.094842</td>\n",
       "      <td>{'mae': 0.8415683551446388}</td>\n",
       "      <td>{'mse': 1.0446256542651344}</td>\n",
       "      <td>{'pearsonr': 0.8938849949933865}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.962456</td>\n",
       "      <td>{'mae': 0.7814070760713596}</td>\n",
       "      <td>{'mse': 0.978784323429759}</td>\n",
       "      <td>{'pearsonr': 0.9029220502713722}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.051039</td>\n",
       "      <td>{'mae': 0.8335375463406213}</td>\n",
       "      <td>{'mse': 1.0232142276477534}</td>\n",
       "      <td>{'pearsonr': 0.9048579458031762}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.905400</td>\n",
       "      <td>{'mae': 0.7630247408033985}</td>\n",
       "      <td>{'mse': 0.9501735262635784}</td>\n",
       "      <td>{'pearsonr': 0.9091407362503119}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.970084</td>\n",
       "      <td>{'mae': 0.795725872083322}</td>\n",
       "      <td>{'mse': 0.9832555402272644}</td>\n",
       "      <td>{'pearsonr': 0.9099291961431453}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.796254</td>\n",
       "      <td>{'mae': 0.7167641626425871}</td>\n",
       "      <td>{'mse': 0.8898961205682184}</td>\n",
       "      <td>{'pearsonr': 0.9210273545342105}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.938942</td>\n",
       "      <td>{'mae': 0.7846219839626237}</td>\n",
       "      <td>{'mse': 0.9675546537784352}</td>\n",
       "      <td>{'pearsonr': 0.9148802855413953}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.900537</td>\n",
       "      <td>{'mae': 0.7677313631890635}</td>\n",
       "      <td>{'mse': 0.9467754991375609}</td>\n",
       "      <td>{'pearsonr': 0.9202748913739829}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.943096</td>\n",
       "      <td>{'mae': 0.7877842209056805}</td>\n",
       "      <td>{'mse': 0.9702828495489344}</td>\n",
       "      <td>{'pearsonr': 0.9170450809194082}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.928947</td>\n",
       "      <td>{'mae': 0.7815563192493037}</td>\n",
       "      <td>{'mse': 0.9629935334336214}</td>\n",
       "      <td>{'pearsonr': 0.9180903846731977}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.915632</td>\n",
       "      <td>{'mae': 0.7733123409512795}</td>\n",
       "      <td>{'mse': 0.9548868601127197}</td>\n",
       "      <td>{'pearsonr': 0.9211238715905823}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.845096</td>\n",
       "      <td>{'mae': 0.7458175996024358}</td>\n",
       "      <td>{'mse': 0.9176890079103781}</td>\n",
       "      <td>{'pearsonr': 0.9240606618347567}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.852048</td>\n",
       "      <td>{'mae': 0.7510169683269372}</td>\n",
       "      <td>{'mse': 0.9213131675751619}</td>\n",
       "      <td>{'pearsonr': 0.926617028243838}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.887345</td>\n",
       "      <td>{'mae': 0.7678521070825426}</td>\n",
       "      <td>{'mse': 0.9407106722440624}</td>\n",
       "      <td>{'pearsonr': 0.9252602670089237}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.893597</td>\n",
       "      <td>{'mae': 0.7633780261403635}</td>\n",
       "      <td>{'mse': 0.9437861035998943}</td>\n",
       "      <td>{'pearsonr': 0.9247202727681207}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.843435</td>\n",
       "      <td>{'mae': 0.7435048106420589}</td>\n",
       "      <td>{'mse': 0.9168497682287783}</td>\n",
       "      <td>{'pearsonr': 0.9282524218754475}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.770595</td>\n",
       "      <td>{'mae': 0.7090807459464199}</td>\n",
       "      <td>{'mse': 0.8762622321987805}</td>\n",
       "      <td>{'pearsonr': 0.9310594536411727}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.741454</td>\n",
       "      <td>{'mae': 0.6944529507554283}</td>\n",
       "      <td>{'mse': 0.8594867479014334}</td>\n",
       "      <td>{'pearsonr': 0.9315411738423729}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.805813</td>\n",
       "      <td>{'mae': 0.7299972206844311}</td>\n",
       "      <td>{'mse': 0.8968956366609974}</td>\n",
       "      <td>{'pearsonr': 0.9304744381865239}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.780097</td>\n",
       "      <td>{'mae': 0.720222047961464}</td>\n",
       "      <td>{'mse': 0.88209656915734}</td>\n",
       "      <td>{'pearsonr': 0.9336046353017753}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>{'mae': 0.6664204978874247}</td>\n",
       "      <td>{'mse': 0.8233406647882276}</td>\n",
       "      <td>{'pearsonr': 0.9370408541333228}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.737809</td>\n",
       "      <td>{'mae': 0.6953497577654688}</td>\n",
       "      <td>{'mse': 0.8574907195524771}</td>\n",
       "      <td>{'pearsonr': 0.9345125851784973}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.747407</td>\n",
       "      <td>{'mae': 0.6953218441260488}</td>\n",
       "      <td>{'mse': 0.8630004255385346}</td>\n",
       "      <td>{'pearsonr': 0.9341849347459144}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.708195</td>\n",
       "      <td>{'mae': 0.6756779418817084}</td>\n",
       "      <td>{'mse': 0.8399828178366784}</td>\n",
       "      <td>{'pearsonr': 0.9364862182733285}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.712483</td>\n",
       "      <td>{'mae': 0.6801339322210926}</td>\n",
       "      <td>{'mse': 0.8424269073621878}</td>\n",
       "      <td>{'pearsonr': 0.9376933658636217}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.741172</td>\n",
       "      <td>{'mae': 0.6895160736614152}</td>\n",
       "      <td>{'mse': 0.8593386787267284}</td>\n",
       "      <td>{'pearsonr': 0.9360265252693512}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.743900</td>\n",
       "      <td>{'mae': 0.6917923140976774}</td>\n",
       "      <td>{'mse': 0.861232168576919}</td>\n",
       "      <td>{'pearsonr': 0.9362306363064299}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.726154</td>\n",
       "      <td>{'mae': 0.6831081637426426}</td>\n",
       "      <td>{'mse': 0.8506726280131213}</td>\n",
       "      <td>{'pearsonr': 0.936871655606957}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.739356</td>\n",
       "      <td>{'mae': 0.689725987162245}</td>\n",
       "      <td>{'mse': 0.8585740615728286}</td>\n",
       "      <td>{'pearsonr': 0.9365524641290564}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.775084</td>\n",
       "      <td>{'mae': 0.7113810448123045}</td>\n",
       "      <td>{'mse': 0.879293306108946}</td>\n",
       "      <td>{'pearsonr': 0.9367753219926815}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.722189</td>\n",
       "      <td>{'mae': 0.6825203727244546}</td>\n",
       "      <td>{'mse': 0.8484117875807387}</td>\n",
       "      <td>{'pearsonr': 0.9384124620573745}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.208000</td>\n",
       "      <td>0.757688</td>\n",
       "      <td>{'mae': 0.7008197373465488}</td>\n",
       "      <td>{'mse': 0.8693402558733488}</td>\n",
       "      <td>{'pearsonr': 0.9373751169506075}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.736395</td>\n",
       "      <td>{'mae': 0.6868484084151293}</td>\n",
       "      <td>{'mse': 0.8569838652598898}</td>\n",
       "      <td>{'pearsonr': 0.9378541677410761}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.744188</td>\n",
       "      <td>{'mae': 0.6900127588918334}</td>\n",
       "      <td>{'mse': 0.8613792657066557}</td>\n",
       "      <td>{'pearsonr': 0.9382520999937438}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.718646</td>\n",
       "      <td>{'mae': 0.6791375859395454}</td>\n",
       "      <td>{'mse': 0.8465675693925921}</td>\n",
       "      <td>{'pearsonr': 0.9384986251252107}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.736576</td>\n",
       "      <td>{'mae': 0.6875483580227745}</td>\n",
       "      <td>{'mse': 0.8570172804075997}</td>\n",
       "      <td>{'pearsonr': 0.9383972098725472}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.779033</td>\n",
       "      <td>{'mae': 0.7111485317055332}</td>\n",
       "      <td>{'mse': 0.8815250799880061}</td>\n",
       "      <td>{'pearsonr': 0.9377320311651783}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.750781</td>\n",
       "      <td>{'mae': 0.6969410219192995}</td>\n",
       "      <td>{'mse': 0.8651798144473589}</td>\n",
       "      <td>{'pearsonr': 0.9387238424028295}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.723778</td>\n",
       "      <td>{'mae': 0.6826666823833396}</td>\n",
       "      <td>{'mse': 0.8493680499084785}</td>\n",
       "      <td>{'pearsonr': 0.9392893092481758}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.724097</td>\n",
       "      <td>{'mae': 0.682734526968316}</td>\n",
       "      <td>{'mse': 0.8495530494248488}</td>\n",
       "      <td>{'pearsonr': 0.9392854592389983}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.727235</td>\n",
       "      <td>{'mae': 0.6845587648843464}</td>\n",
       "      <td>{'mse': 0.8514263422019855}</td>\n",
       "      <td>{'pearsonr': 0.9391638892249982}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-500\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-500\\pytorch_model.bin\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-1000\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1/checkpoint-1000\\pytorch_model.bin\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=1.1795211601257325, metrics={'train_runtime': 171.8749, 'train_samples_per_second': 222.255, 'train_steps_per_second': 6.982, 'total_flos': 351966825062400.0, 'train_loss': 1.1795211601257325, 'epoch': 50.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c152ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#take care of distributed/paralelle training \n",
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model \n",
    "model_to_save.save_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fb37c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.109,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.144,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 464,\n",
      "  \"is_gpu\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 515,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"norm_mean\": [\n",
      "    11.199569164274653,\n",
      "    -0.9728601944583675,\n",
      "    11.199595401578872,\n",
      "    0.1914454376660732,\n",
      "    0.608589373135307,\n",
      "    365.064017672,\n",
      "    342.24912812000014,\n",
      "    364.6033136038417,\n",
      "    134.06547,\n",
      "    0.004249,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    1.1861084842221647,\n",
      "    1.890967178564785,\n",
      "    2.519587985439997,\n",
      "    2.0112818114267816,\n",
      "    795.5621221754437,\n",
      "    18.14439203724506,\n",
      "    14.536240385432393,\n",
      "    15.215140271072487,\n",
      "    12.068994414289726,\n",
      "    8.453657900068215,\n",
      "    9.114162139055054,\n",
      "    6.434168605708085,\n",
      "    7.215103879809845,\n",
      "    4.436200487997215,\n",
      "    5.109730699855831,\n",
      "    3.055231525907226,\n",
      "    3.6252747118486264,\n",
      "    -2.202564923376624,\n",
      "    18.195385007867852,\n",
      "    7.9706993589944775,\n",
      "    4.5379164631837545,\n",
      "    150.95250337667272,\n",
      "    13.184208966483704,\n",
      "    8.814008658052902,\n",
      "    3.8191839078987306,\n",
      "    3.4969386790830774,\n",
      "    2.9222201316693712,\n",
      "    2.644444123964607,\n",
      "    6.408740449956927,\n",
      "    4.95314480536345,\n",
      "    2.6263770771853108,\n",
      "    2.4113616526384853,\n",
      "    26.24052195128434,\n",
      "    37.102909834641714,\n",
      "    19.89943953042712,\n",
      "    16.353848799228413,\n",
      "    15.638332143998122,\n",
      "    21.706094849865753,\n",
      "    0.28727529762970366,\n",
      "    8.054432014422119,\n",
      "    3.2648099385428853,\n",
      "    32.629006626588726,\n",
      "    16.26551059790217,\n",
      "    47.70605007162041,\n",
      "    0.0,\n",
      "    5.325837027308287,\n",
      "    9.698460925314944,\n",
      "    5.573601891254677,\n",
      "    2.581492771453006,\n",
      "    7.3124961943884665,\n",
      "    33.07539073817076,\n",
      "    10.718462271839512,\n",
      "    6.99277406210818,\n",
      "    31.684923475431933,\n",
      "    36.92162447084414,\n",
      "    1.2074202610211657,\n",
      "    5.110701506051421,\n",
      "    0.0,\n",
      "    71.04050338999998,\n",
      "    9.57750975344203,\n",
      "    10.066085526965992,\n",
      "    0.07691213090851719,\n",
      "    13.38923196114951,\n",
      "    16.862422387837878,\n",
      "    21.382953923695233,\n",
      "    15.651918121909311,\n",
      "    14.440634953378058,\n",
      "    19.13130604146014,\n",
      "    22.114944705243296,\n",
      "    8.183429061888226,\n",
      "    13.699768012021506,\n",
      "    2.1212691930096144,\n",
      "    17.474216494453906,\n",
      "    7.8467696174922725,\n",
      "    2.6683841482907034,\n",
      "    0.11868201225906093,\n",
      "    9.064881467380093,\n",
      "    2.659801877718109,\n",
      "    4.055917032498944,\n",
      "    0.259848432909807,\n",
      "    0.413963629624058,\n",
      "    25.186704,\n",
      "    1.79722,\n",
      "    5.353545,\n",
      "    0.272499,\n",
      "    0.562898,\n",
      "    0.835397,\n",
      "    1.236854,\n",
      "    0.729917,\n",
      "    1.966771,\n",
      "    4.216321,\n",
      "    1.414081,\n",
      "    6.486208,\n",
      "    5.688314,\n",
      "    0.205632,\n",
      "    0.409204,\n",
      "    0.614836,\n",
      "    2.802168,\n",
      "    2.7549044689500004,\n",
      "    97.31541557350002,\n",
      "    0.069051,\n",
      "    0.151924,\n",
      "    0.130758,\n",
      "    0.06279,\n",
      "    0.027038,\n",
      "    0.999062,\n",
      "    0.096951,\n",
      "    0.042862,\n",
      "    0.096089,\n",
      "    0.100163,\n",
      "    1.033857,\n",
      "    1.034286,\n",
      "    0.016206,\n",
      "    0.00357,\n",
      "    0.016776,\n",
      "    1.488795,\n",
      "    0.915699,\n",
      "    0.232236,\n",
      "    0.012241,\n",
      "    0.074885,\n",
      "    0.131561,\n",
      "    0.096951,\n",
      "    0.004026,\n",
      "    0.009835,\n",
      "    0.011646,\n",
      "    0.250196,\n",
      "    0.131237,\n",
      "    0.768633,\n",
      "    0.015927,\n",
      "    0.539599,\n",
      "    0.451885,\n",
      "    0.001726,\n",
      "    0.003335,\n",
      "    0.001218,\n",
      "    1.236474,\n",
      "    0.000226,\n",
      "    0.555529,\n",
      "    0.000149,\n",
      "    0.001046,\n",
      "    0.002578,\n",
      "    0.126995,\n",
      "    0.732216,\n",
      "    0.037978,\n",
      "    0.019179,\n",
      "    0.720141,\n",
      "    0.018951,\n",
      "    0.013025,\n",
      "    0.059523,\n",
      "    0.027553,\n",
      "    0.000831,\n",
      "    0.0002,\n",
      "    0.073914,\n",
      "    0.061694,\n",
      "    0.002249,\n",
      "    0.007716,\n",
      "    0.236426,\n",
      "    0.0287,\n",
      "    0.05231,\n",
      "    0.041425,\n",
      "    0.033421,\n",
      "    0.017275,\n",
      "    0.001082,\n",
      "    0.011915,\n",
      "    0.004249,\n",
      "    0.196769,\n",
      "    0.039316,\n",
      "    0.038686,\n",
      "    0.00409,\n",
      "    0.003615,\n",
      "    0.116124,\n",
      "    0.051192,\n",
      "    0.025177,\n",
      "    0.0,\n",
      "    0.161908,\n",
      "    0.315775,\n",
      "    0.087229,\n",
      "    0.079586,\n",
      "    0.023227,\n",
      "    0.005966,\n",
      "    0.007901,\n",
      "    0.050376,\n",
      "    0.000186,\n",
      "    0.065723,\n",
      "    0.380193,\n",
      "    0.051566\n",
      "  ],\n",
      "  \"norm_std\": [\n",
      "    2.9210526350021033,\n",
      "    1.5294133532822065,\n",
      "    2.9209947673330334,\n",
      "    0.21956154740898992,\n",
      "    0.22097666681598954,\n",
      "    160.48566423804579,\n",
      "    151.38170855657367,\n",
      "    160.3304390667665,\n",
      "    60.484857692625106,\n",
      "    0.181038611279414,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.24851193112366385,\n",
      "    0.317494124851492,\n",
      "    0.37175815103599535,\n",
      "    0.6098706561111424,\n",
      "    539.8195290502504,\n",
      "    8.140940922894863,\n",
      "    6.600767667198695,\n",
      "    6.700942921964325,\n",
      "    5.536318526756788,\n",
      "    4.020569431789569,\n",
      "    4.316039675035455,\n",
      "    3.229701298304296,\n",
      "    4.058753110098356,\n",
      "    2.399274478688092,\n",
      "    4.590084765547685,\n",
      "    1.8657465201411236,\n",
      "    8.197075845395899,\n",
      "    1.3989800795766576,\n",
      "    8.727770321711972,\n",
      "    4.719034225006412,\n",
      "    3.6844834579923407,\n",
      "    66.65125255607474,\n",
      "    11.022808176926917,\n",
      "    9.88512023443511,\n",
      "    5.895101555004671,\n",
      "    6.0315631910071374,\n",
      "    4.465786134186721,\n",
      "    8.73293454096314,\n",
      "    7.292192943139112,\n",
      "    5.798809757257198,\n",
      "    5.458840154330179,\n",
      "    5.34562222799046,\n",
      "    28.624753237838462,\n",
      "    22.7685485030176,\n",
      "    13.735506972569182,\n",
      "    12.75558914023291,\n",
      "    12.647297666063738,\n",
      "    16.73803715869515,\n",
      "    1.3236865505015507,\n",
      "    8.012917117258175,\n",
      "    6.328266302270954,\n",
      "    30.80439768300023,\n",
      "    14.510669158473307,\n",
      "    33.76748799216324,\n",
      "    0.0,\n",
      "    8.851153866015428,\n",
      "    8.222102882220607,\n",
      "    7.329351085680612,\n",
      "    4.87773057457412,\n",
      "    10.796349487508557,\n",
      "    24.55359833254403,\n",
      "    10.33295824604808,\n",
      "    8.986884190324291,\n",
      "    26.77991276665104,\n",
      "    29.521288543995215,\n",
      "    4.077418430037268,\n",
      "    11.23487898363004,\n",
      "    0.0,\n",
      "    50.277243284807206,\n",
      "    19.12173183245714,\n",
      "    9.819697177666312,\n",
      "    1.4201437981599128,\n",
      "    12.511435257208836,\n",
      "    14.212538029397628,\n",
      "    16.973978925056553,\n",
      "    19.21649041911615,\n",
      "    15.092240504961104,\n",
      "    19.889237093009676,\n",
      "    25.80872442073538,\n",
      "    9.254317550453825,\n",
      "    19.013243564373347,\n",
      "    3.6841568734614953,\n",
      "    17.690679185577395,\n",
      "    10.27595457263499,\n",
      "    3.3283202642652645,\n",
      "    2.8773795244438474,\n",
      "    9.228734822190495,\n",
      "    5.106296483962912,\n",
      "    4.008127533955226,\n",
      "    2.3345092198667503,\n",
      "    0.23958883840178574,\n",
      "    11.48532061063049,\n",
      "    2.0042680181777808,\n",
      "    3.411142707197923,\n",
      "    0.7103265443180337,\n",
      "    0.8009597262862117,\n",
      "    1.0630493791282618,\n",
      "    1.2495037990913607,\n",
      "    0.8592211073826755,\n",
      "    1.4909738617970663,\n",
      "    2.8049912821495706,\n",
      "    1.5692082041123125,\n",
      "    3.7188860712382157,\n",
      "    4.918753910447648,\n",
      "    0.6213838320183964,\n",
      "    0.6971589290933399,\n",
      "    0.9385507839118636,\n",
      "    1.7370945619837506,\n",
      "    2.7759468746763334,\n",
      "    43.91556441471313,\n",
      "    0.2929625321198007,\n",
      "    0.6742399816263887,\n",
      "    0.6447563579731193,\n",
      "    0.26136083143708466,\n",
      "    0.1703202147866646,\n",
      "    1.3696411924562566,\n",
      "    0.3394696140137124,\n",
      "    0.26977939457438505,\n",
      "    0.3350074869447194,\n",
      "    0.3408584597974497,\n",
      "    1.2690580420372088,\n",
      "    1.2684116362885036,\n",
      "    0.1297126917051003,\n",
      "    0.06304965563156611,\n",
      "    0.17914965229828922,\n",
      "    1.485673805113914,\n",
      "    1.1656052934139842,\n",
      "    0.5018632205797633,\n",
      "    0.15576643470973517,\n",
      "    0.2883562378800223,\n",
      "    0.3774901929558512,\n",
      "    0.3394696140137124,\n",
      "    0.07983606764988928,\n",
      "    0.10307416455777559,\n",
      "    0.11692041889415362,\n",
      "    1.0010868912132271,\n",
      "    0.7705779932112281,\n",
      "    1.157481598590082,\n",
      "    0.13507534533122212,\n",
      "    0.8359812306885952,\n",
      "    0.7600865243553028,\n",
      "    0.04757124327808961,\n",
      "    0.07183232513905516,\n",
      "    0.03513570421263404,\n",
      "    1.239225396368063,\n",
      "    0.015097985029438593,\n",
      "    1.3364349277900949,\n",
      "    0.013378265133341392,\n",
      "    0.032663541616103894,\n",
      "    0.060970137226002974,\n",
      "    0.44400840883756576,\n",
      "    1.159532265122051,\n",
      "    0.198246590935912,\n",
      "    0.1491817288215558,\n",
      "    1.28126795861232,\n",
      "    0.143114919141507,\n",
      "    0.11579880303510387,\n",
      "    0.25012811724209466,\n",
      "    0.1830406121462275,\n",
      "    0.03504726333553974,\n",
      "    0.015295758691880374,\n",
      "    0.3034514997274073,\n",
      "    0.2749689545601939,\n",
      "    0.04859983910409953,\n",
      "    0.09878498419533764,\n",
      "    0.5707110234042025,\n",
      "    0.17028898672063034,\n",
      "    0.24456026600763192,\n",
      "    0.21322057789532142,\n",
      "    0.1917343827305721,\n",
      "    0.13591391704896466,\n",
      "    0.03519702423260403,\n",
      "    0.11080182783711219,\n",
      "    0.0680510883818226,\n",
      "    0.5264724473438641,\n",
      "    0.2602735481879015,\n",
      "    0.25847912916802446,\n",
      "    0.10886360159063149,\n",
      "    0.10026934640727359,\n",
      "    0.35113436163289397,\n",
      "    0.2260341350934195,\n",
      "    0.16874580630684471,\n",
      "    0.0,\n",
      "    0.4146998571400424,\n",
      "    0.5347143492505464,\n",
      "    0.3137422508894841,\n",
      "    0.27962501103110715,\n",
      "    0.1547563582555832,\n",
      "    0.08130444916739461,\n",
      "    0.08949068223889126,\n",
      "    0.22530492534853602,\n",
      "    0.014421012861987593,\n",
      "    0.2736413019822887,\n",
      "    2.253629375384596,\n",
      "    0.22817317920167496\n",
      "  ],\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 600\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at C:/Users/pj11/Documents/bert_finetune2/esol_fold1//model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 190\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making prediction \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_output_path)\n",
    "\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "     output_dir =model_output_path,\n",
    "     do_train = False,\n",
    "     do_predict = True,\n",
    "     dataloader_drop_last = False\n",
    ")\n",
    "\n",
    "# Init Trainer\n",
    "trainer=Trainer(\n",
    "          model = model,\n",
    "          args = test_args,\n",
    "          compute_metrics = compute_metrics)\n",
    "\n",
    "test_results = trainer.predict(small_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27ef5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61 , -3.083,  0.38 , -1.99 , -6.18 , -4.799, -2.08 , -4.64 ,\n",
       "       -1.29 , -2.484, -1.37 , -1.06 , -4.345, -0.98 , -2.41 , -3.88 ,\n",
       "       -2.68 , -3.38 , -7.68 , -2.77 , -2.11 , -3.36 , -2.266, -4.23 ,\n",
       "       -0.67 ,  0.94 , -3.39 , -2.369, -2.58 , -0.5  , -1.13 , -9.15 ,\n",
       "       -7.96 , -3.77 , -4.24 , -1.62 , -4.14 , -1.877, -0.4  , -2.92 ,\n",
       "       -1.25 , -1.85 , -4.47 , -1.09 , -2.37 , -4.873, -3.03 , -4.411,\n",
       "       -1.92 , -2.03 , -3.504, -3.78 , -2.461, -1.52 , -0.59 , -3.23 ,\n",
       "       -1.24 , -5.67 , -2.982, -5.736, -2.843, -1.49 , -2.57 , -9.332,\n",
       "       -3.   , -2.22 , -4.11 , -4.37 , -2.35 , -1.33 , -6.01 , -6.27 ,\n",
       "       -1.91 , -4.56 , -0.8  , -0.4  , -3.094, -3.4  , -2.25 , -0.12 ,\n",
       "       -1.72 , -3.324, -0.59 , -0.99 , -3.893, -4.955, -0.74 , -1.06 ,\n",
       "       -4.13 , -7.43 , -2.93 , -9.16 , -0.6  , -0.8  , -3.7  , -2.943,\n",
       "       -3.37 , -0.92 , -2.39 , -3.24 , -6.29 , -1.708, -4.48 , -6.26 ,\n",
       "       -2.67 , -4.679, -2.253, -3.17 , -7.337, -5.184, -2.468, -6.14 ,\n",
       "       -7.32 , -4.71 , -0.72 , -2.68 , -3.42 ,  0.1  , -1.6  , -5.46 ,\n",
       "       -2.596, -0.72 , -3.79 , -3.8  , -1.11 , -3.246, -1.87 , -0.63 ,\n",
       "       -1.28 , -1.456, -1.23 ,  0.62 , -5.284, -1.8  , -3.3  , -5.259,\n",
       "       -0.44 , -4.743, -3.3  , -0.9  , -2.253, -3.09 , -1.41 , -0.41 ,\n",
       "       -0.85 , -0.77 , -2.56 , -1.45 , -4.9  , -5.05 , -4.35 , -0.807,\n",
       "       -2.82 , -4.445, -3.27 , -5.4  , -2.43 , -4.7  , -0.03 , -2.436,\n",
       "       -2.42 , -1.36 , -2.38 , -3.38 , -2.05 , -3.18 ,  0.7  , -3.15 ,\n",
       "       -8.176, -0.73 , -2.831, -1.21 , -3.592, -4.8  , -3.35 , -3.796,\n",
       "       -3.68 , -4.3  , -2.64 , -3.68 , -1.17 , -3.8  , -0.28 , -3.14 ,\n",
       "       -5.65 , -0.36 , -4.07 ,  0.32 , -6.08 , -5.19 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09656a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6118962168693542,\n",
       " 'test_mae': {'mae': 0.6257151697517226},\n",
       " 'test_rmse': {'mse': 0.7844339503206982},\n",
       " 'test_pearsonr': {'pearsonr': 0.9451561078776958},\n",
       " 'test_runtime': 0.9692,\n",
       " 'test_samples_per_second': 196.036,\n",
       " 'test_steps_per_second': 6.191}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "362ec747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62571514\n",
      "0.7844339340988442\n",
      "0.8490492765539396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "\n",
    "# MAE, AE and RMSE give an idea of the error distribution\n",
    "print(mean_absolute_error(test_results[0], test_results[1]))\n",
    "\n",
    "#RMSEs\n",
    "print(math.sqrt(mean_squared_error(test_results[0], test_results[1])))\n",
    "\n",
    "# R^2 Coefficient of Determination\n",
    "print(r2_score(test_results[0], test_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef967db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred. log S')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "\n",
    "ln = np.arange(-10, 5, 0.2)\n",
    "plt.plot(ln, ln,'r--')\n",
    "plt.scatter(test_results[1], test_results[0])\n",
    "plt.xlabel('exp. log S')\n",
    "plt.ylabel('pred. log S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9c240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_pred_test=pd.DataFrame(test_results[0], columns=[\"predict\"])\n",
    "pd_exp_test=pd.DataFrame(test_results[1], columns=[\"exp\"])\n",
    "pd_smiles=pd.DataFrame(dataset['test']['smiles'], columns=[\"smiles\"])\n",
    "pd_test=pd.concat((pd_smiles, pd_exp_test, pd_pred_test), axis=1)\n",
    "\n",
    "# save predicton to csv \n",
    "pd_test.to_csv('esol_bert_ds6_fold1_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5108c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04564f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
