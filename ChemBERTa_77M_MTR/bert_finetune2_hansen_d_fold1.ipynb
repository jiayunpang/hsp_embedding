{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0df1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c986d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e7fa4e50daa5676d\n",
      "Found cached dataset csv (C:/Users/pj11/.cache/huggingface/datasets/csv/default-e7fa4e50daa5676d/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50765873c57a42d0a699b715fc35b037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the code is adapted from the \"fine-tuning a pretrained model\" and\n",
    "# \"fine-tuning a model with the trainer API\" course and examples on hugging face\n",
    "# check huggingface for further explanations\n",
    "\n",
    "# load data into training, valdidation and test\n",
    "dataset = load_dataset('csv', data_files={'train':['hansen_d_bert_ds1.csv', 'hansen_d_bert_ds2.csv',\n",
    "                                                   'hansen_d_bert_ds3.csv', 'hansen_d_bert_ds4.csv'],\n",
    "                                          'validation':'hansen_d_bert_ds5.csv',\n",
    "                                          'test': 'hansen_d_bert_ds6.csv'}, delimiter=',', column_names =['smiles', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9874b270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles': 'CC\\\\C(C)=N\\\\O', 'label': 14.7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data format\n",
    "dataset['validation'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9894c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# specify model from hugging face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepchem/ChemBERTa-77M-MTR\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"smiles\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b48772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\pj11\\.cache\\huggingface\\datasets\\csv\\default-e7fa4e50daa5676d\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-8e629755c8a0e61c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\pj11\\.cache\\huggingface\\datasets\\csv\\default-e7fa4e50daa5676d\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-afe19ba60bc582e8.arrow\n",
      "Loading cached processed dataset at C:\\Users\\pj11\\.cache\\huggingface\\datasets\\csv\\default-e7fa4e50daa5676d\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-ebb54aff268407bd.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009f538e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 197\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['smiles', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 197\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f040040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=random_state).select(range(1000))\n",
    "#small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=random_state).select(range(1000))\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"]\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"]\n",
    "small_test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b5f09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepchem/ChemBERTa-77M-MTR were not used when initializing RobertaForSequenceClassification: ['regression.out_proj.weight', 'regression.dense.bias', 'norm_std', 'norm_mean', 'regression.out_proj.bias', 'regression.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at deepchem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# for regression, num_labels=1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"deepchem/ChemBERTa-77M-MTR\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90cab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric\n",
    "mae_metric = evaluate.load(\"mae\")\n",
    "mse_metric = evaluate.load(\"mse\")\n",
    "pearsonr_metric = evaluate.load(\"pearsonr\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    #logits, labels = eval_pred\n",
    "    #predictions = np.argmax(logits, axis=-1)\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics.update({'mae': mae_metric.compute(predictions=predictions, references=labels)})\n",
    "    metrics.update({'rmse': mse_metric.compute(predictions=predictions, references=labels, squared=False)})\n",
    "    metrics.update({'pearsonr': pearsonr_metric.compute(predictions=predictions, references=labels)})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574f2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the finetuned model\n",
    "para_output_dir = 'C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/'\n",
    "model_output_path = f'{para_output_dir}/model'\n",
    "\n",
    "# specify trainining arguments \n",
    "training_args = TrainingArguments(output_dir=para_output_dir, \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size = 4,\n",
    "                                  per_device_eval_batch_size = 4,\n",
    "                                  num_train_epochs=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4510db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "794a8620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 789\n",
      "  Num Epochs = 70\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3500\n",
      "  Number of trainable parameters = 3427825\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 06:26, Epoch 70/70]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Pearsonr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>228.964752</td>\n",
       "      <td>{'mae': 14.979710974593454}</td>\n",
       "      <td>{'mse': 15.135946424237694}</td>\n",
       "      <td>{'pearsonr': -0.3045547599299149}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>103.159172</td>\n",
       "      <td>{'mae': 9.97893736689224}</td>\n",
       "      <td>{'mse': 10.160337065373533}</td>\n",
       "      <td>{'pearsonr': 0.021224954714146715}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>76.303032</td>\n",
       "      <td>{'mae': 8.5359614875716}</td>\n",
       "      <td>{'mse': 8.737992423183332}</td>\n",
       "      <td>{'pearsonr': -0.02431046182088696}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>58.452232</td>\n",
       "      <td>{'mae': 7.418146419041048}</td>\n",
       "      <td>{'mse': 7.648217882517792}</td>\n",
       "      <td>{'pearsonr': -0.025552238557137872}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>44.309059</td>\n",
       "      <td>{'mae': 6.394450081181406}</td>\n",
       "      <td>{'mse': 6.65935031315379}</td>\n",
       "      <td>{'pearsonr': -0.02362946255468576}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>32.828758</td>\n",
       "      <td>{'mae': 5.422941546754789}</td>\n",
       "      <td>{'mse': 5.732552060926986}</td>\n",
       "      <td>{'pearsonr': -0.020798256322622465}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>23.563627</td>\n",
       "      <td>{'mae': 4.498151508079568}</td>\n",
       "      <td>{'mse': 4.85724970932887}</td>\n",
       "      <td>{'pearsonr': -0.017153802587210155}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>16.247843</td>\n",
       "      <td>{'mae': 3.615772339293194}</td>\n",
       "      <td>{'mse': 4.033997419714882}</td>\n",
       "      <td>{'pearsonr': -0.013756303344141074}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.712799</td>\n",
       "      <td>{'mae': 2.7702804458927988}</td>\n",
       "      <td>{'mse': 3.2763231277518186}</td>\n",
       "      <td>{'pearsonr': -0.008800079646650836}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>6.817681</td>\n",
       "      <td>{'mae': 2.0239434266453467}</td>\n",
       "      <td>{'mse': 2.614476935491016}</td>\n",
       "      <td>{'pearsonr': -0.0022682795254390965}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>4.472268</td>\n",
       "      <td>{'mae': 1.5906845131501328}</td>\n",
       "      <td>{'mse': 2.1181423498371994}</td>\n",
       "      <td>{'pearsonr': 0.006518192452917003}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>3.612229</td>\n",
       "      <td>{'mae': 1.5104920827797828}</td>\n",
       "      <td>{'mse': 1.9031985546137222}</td>\n",
       "      <td>{'pearsonr': 0.14644720238172987}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>2.973798</td>\n",
       "      <td>{'mae': 1.3219275256703953}</td>\n",
       "      <td>{'mse': 1.72778977278765}</td>\n",
       "      <td>{'pearsonr': 0.6408553761090081}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>2.368738</td>\n",
       "      <td>{'mae': 1.1350092427984713}</td>\n",
       "      <td>{'mse': 1.5413302834240041}</td>\n",
       "      <td>{'pearsonr': 0.6680451565146015}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>1.950900</td>\n",
       "      <td>{'mae': 1.0380480180537035}</td>\n",
       "      <td>{'mse': 1.4012786426031256}</td>\n",
       "      <td>{'pearsonr': 0.783780322962077}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>1.515092</td>\n",
       "      <td>{'mae': 0.9110703540937549}</td>\n",
       "      <td>{'mse': 1.2363036660747757}</td>\n",
       "      <td>{'pearsonr': 0.8045118194241697}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>1.405844</td>\n",
       "      <td>{'mae': 0.8467361818110277}</td>\n",
       "      <td>{'mse': 1.1895601427003775}</td>\n",
       "      <td>{'pearsonr': 0.8206910133584647}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>1.393971</td>\n",
       "      <td>{'mae': 0.802053349877372}</td>\n",
       "      <td>{'mse': 1.1833822615517962}</td>\n",
       "      <td>{'pearsonr': 0.7866516022981601}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>72.035500</td>\n",
       "      <td>1.180725</td>\n",
       "      <td>{'mae': 0.7622820471749088}</td>\n",
       "      <td>{'mse': 1.0906217954405943}</td>\n",
       "      <td>{'pearsonr': 0.8246299416102112}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.172102</td>\n",
       "      <td>{'mae': 0.7436506832916725}</td>\n",
       "      <td>{'mse': 1.0863007388331072}</td>\n",
       "      <td>{'pearsonr': 0.8186650870680324}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.224993</td>\n",
       "      <td>{'mae': 0.7718409911024994}</td>\n",
       "      <td>{'mse': 1.1084619815673133}</td>\n",
       "      <td>{'pearsonr': 0.8051376123738017}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.277413</td>\n",
       "      <td>{'mae': 0.7501953870511903}</td>\n",
       "      <td>{'mse': 1.1323976549859218}</td>\n",
       "      <td>{'pearsonr': 0.8070966750014568}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.204153</td>\n",
       "      <td>{'mae': 0.7585870336154996}</td>\n",
       "      <td>{'mse': 1.0995321930688529}</td>\n",
       "      <td>{'pearsonr': 0.8077821113230932}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.399214</td>\n",
       "      <td>{'mae': 0.7628194257087514}</td>\n",
       "      <td>{'mse': 1.1843311940775942}</td>\n",
       "      <td>{'pearsonr': 0.7912401135160787}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.237538</td>\n",
       "      <td>{'mae': 0.7372862191369691}</td>\n",
       "      <td>{'mse': 1.1139246508369378}</td>\n",
       "      <td>{'pearsonr': 0.800366069359121}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.297286</td>\n",
       "      <td>{'mae': 0.749845451509892}</td>\n",
       "      <td>{'mse': 1.1396631520709906}</td>\n",
       "      <td>{'pearsonr': 0.7918895228135014}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.187800</td>\n",
       "      <td>{'mae': 0.7288960393915321}</td>\n",
       "      <td>{'mse': 1.0907774737890779}</td>\n",
       "      <td>{'pearsonr': 0.8179319341963366}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.157743</td>\n",
       "      <td>{'mae': 0.7348689355220891}</td>\n",
       "      <td>{'mse': 1.0765637473763954}</td>\n",
       "      <td>{'pearsonr': 0.815022722546586}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.194900</td>\n",
       "      <td>1.178709</td>\n",
       "      <td>{'mae': 0.7475347034822261}</td>\n",
       "      <td>{'mse': 1.0857847124678446}</td>\n",
       "      <td>{'pearsonr': 0.8115975697601154}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.197715</td>\n",
       "      <td>{'mae': 0.7310333542412307}</td>\n",
       "      <td>{'mse': 1.0952793262211402}</td>\n",
       "      <td>{'pearsonr': 0.8102863574876645}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.125377</td>\n",
       "      <td>{'mae': 0.7327929850157142}</td>\n",
       "      <td>{'mse': 1.061463292699983}</td>\n",
       "      <td>{'pearsonr': 0.8333121567836141}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.166111</td>\n",
       "      <td>{'mae': 0.7170506201419734}</td>\n",
       "      <td>{'mse': 1.0798774887553029}</td>\n",
       "      <td>{'pearsonr': 0.8144709175383913}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.246137</td>\n",
       "      <td>{'mae': 0.7148073651463852}</td>\n",
       "      <td>{'mse': 1.1162628643788486}</td>\n",
       "      <td>{'pearsonr': 0.8055934831516467}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.281519</td>\n",
       "      <td>{'mae': 0.7151315369581813}</td>\n",
       "      <td>{'mse': 1.1319983903710815}</td>\n",
       "      <td>{'pearsonr': 0.8008446718345614}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.155341</td>\n",
       "      <td>{'mae': 0.7062300909594231}</td>\n",
       "      <td>{'mse': 1.074472945003645}</td>\n",
       "      <td>{'pearsonr': 0.8170965466104199}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.168369</td>\n",
       "      <td>{'mae': 0.7023450347977847}</td>\n",
       "      <td>{'mse': 1.0805279654225504}</td>\n",
       "      <td>{'pearsonr': 0.8166907104794918}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.125718</td>\n",
       "      <td>{'mae': 0.7021788873043157}</td>\n",
       "      <td>{'mse': 1.0609411903081114}</td>\n",
       "      <td>{'pearsonr': 0.8270087332547759}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.183413</td>\n",
       "      <td>{'mae': 0.707706456257002}</td>\n",
       "      <td>{'mse': 1.0876126012444762}</td>\n",
       "      <td>{'pearsonr': 0.815503539081196}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>1.246326</td>\n",
       "      <td>{'mae': 0.708680404624358}</td>\n",
       "      <td>{'mse': 1.116074620953024}</td>\n",
       "      <td>{'pearsonr': 0.8048833162620146}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.161078</td>\n",
       "      <td>{'mae': 0.7113190036134671}</td>\n",
       "      <td>{'mse': 1.0768259533125877}</td>\n",
       "      <td>{'pearsonr': 0.8162206220280672}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.307475</td>\n",
       "      <td>{'mae': 0.7122554537003416}</td>\n",
       "      <td>{'mse': 1.14288724972517}</td>\n",
       "      <td>{'pearsonr': 0.797506259499523}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.268285</td>\n",
       "      <td>{'mae': 0.7066645210769575}</td>\n",
       "      <td>{'mse': 1.1257899636230293}</td>\n",
       "      <td>{'pearsonr': 0.8043813493333093}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.254228</td>\n",
       "      <td>{'mae': 0.7049609779706462}</td>\n",
       "      <td>{'mse': 1.1193063374884056}</td>\n",
       "      <td>{'pearsonr': 0.8053421019810971}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.214074</td>\n",
       "      <td>{'mae': 0.7029895782470703}</td>\n",
       "      <td>{'mse': 1.1013653737536369}</td>\n",
       "      <td>{'pearsonr': 0.8175006841615255}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.149665</td>\n",
       "      <td>{'mae': 0.6965585214837553}</td>\n",
       "      <td>{'mse': 1.0716595245770004}</td>\n",
       "      <td>{'pearsonr': 0.8256549681401185}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.070262</td>\n",
       "      <td>{'mae': 0.695999663493355}</td>\n",
       "      <td>{'mse': 1.0336506964983383}</td>\n",
       "      <td>{'pearsonr': 0.8340809446722364}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.016443</td>\n",
       "      <td>{'mae': 0.6932278889689953}</td>\n",
       "      <td>{'mse': 1.0072241402676763}</td>\n",
       "      <td>{'pearsonr': 0.8415297437127998}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>1.039830</td>\n",
       "      <td>{'mae': 0.6999497631479641}</td>\n",
       "      <td>{'mse': 1.0189408257602992}</td>\n",
       "      <td>{'pearsonr': 0.8424981791884082}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.990392</td>\n",
       "      <td>{'mae': 0.6886803486625556}</td>\n",
       "      <td>{'mse': 0.9943570233601711}</td>\n",
       "      <td>{'pearsonr': 0.8474110938639448}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.008677</td>\n",
       "      <td>{'mae': 0.6941259742388265}</td>\n",
       "      <td>{'mse': 1.0035507673273825}</td>\n",
       "      <td>{'pearsonr': 0.8458641179589741}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.118305</td>\n",
       "      <td>{'mae': 0.7062601728487741}</td>\n",
       "      <td>{'mse': 1.0566446560073812}</td>\n",
       "      <td>{'pearsonr': 0.8303602194918562}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.071321</td>\n",
       "      <td>{'mae': 0.7023613247169456}</td>\n",
       "      <td>{'mse': 1.0337950084231136}</td>\n",
       "      <td>{'pearsonr': 0.8321738725906307}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.036176</td>\n",
       "      <td>{'mae': 0.6925032901279817}</td>\n",
       "      <td>{'mse': 1.0169157354264051}</td>\n",
       "      <td>{'pearsonr': 0.8404772297149499}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.040157</td>\n",
       "      <td>{'mae': 0.6904253402942329}</td>\n",
       "      <td>{'mse': 1.0189874289913896}</td>\n",
       "      <td>{'pearsonr': 0.8417014398798488}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.046205</td>\n",
       "      <td>{'mae': 0.6890999653617743}</td>\n",
       "      <td>{'mse': 1.0220408073751062}</td>\n",
       "      <td>{'pearsonr': 0.8415770452409503}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>{'mae': 0.6909857183543558}</td>\n",
       "      <td>{'mse': 1.0200892590288408}</td>\n",
       "      <td>{'pearsonr': 0.8381727426624279}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.084972</td>\n",
       "      <td>{'mae': 0.689787985709718}</td>\n",
       "      <td>{'mse': 1.0407837739127066}</td>\n",
       "      <td>{'pearsonr': 0.8337151409548877}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.018696</td>\n",
       "      <td>{'mae': 0.6905010630031527}</td>\n",
       "      <td>{'mse': 1.0084233450583608}</td>\n",
       "      <td>{'pearsonr': 0.8443357393257653}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>1.032433</td>\n",
       "      <td>{'mae': 0.6887397330424507}</td>\n",
       "      <td>{'mse': 1.015209872039789}</td>\n",
       "      <td>{'pearsonr': 0.8415154087110528}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.073114</td>\n",
       "      <td>{'mae': 0.6985070330237374}</td>\n",
       "      <td>{'mse': 1.0353254557661695}</td>\n",
       "      <td>{'pearsonr': 0.8390305193931917}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.095883</td>\n",
       "      <td>{'mae': 0.6931699617259999}</td>\n",
       "      <td>{'mse': 1.0461765657328435}</td>\n",
       "      <td>{'pearsonr': 0.8338265797186996}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.104788</td>\n",
       "      <td>{'mae': 0.6932770491856609}</td>\n",
       "      <td>{'mse': 1.0505334930569001}</td>\n",
       "      <td>{'pearsonr': 0.8331787148066875}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.069936</td>\n",
       "      <td>{'mae': 0.6907717177105434}</td>\n",
       "      <td>{'mse': 1.0336501127808309}</td>\n",
       "      <td>{'pearsonr': 0.8362197139523626}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.090972</td>\n",
       "      <td>{'mae': 0.6942768096923828}</td>\n",
       "      <td>{'mse': 1.0439908641573612}</td>\n",
       "      <td>{'pearsonr': 0.8356644628536594}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.097623</td>\n",
       "      <td>{'mae': 0.693100391910766}</td>\n",
       "      <td>{'mse': 1.0471504978177164}</td>\n",
       "      <td>{'pearsonr': 0.8333966657483121}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.092916</td>\n",
       "      <td>{'mae': 0.6920389620785786}</td>\n",
       "      <td>{'mse': 1.0448647545019867}</td>\n",
       "      <td>{'pearsonr': 0.8332472266946332}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.105873</td>\n",
       "      <td>{'mae': 0.6947303665470956}</td>\n",
       "      <td>{'mse': 1.0511106027608863}</td>\n",
       "      <td>{'pearsonr': 0.8323551732454256}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.121019</td>\n",
       "      <td>{'mae': 0.6974797321455127}</td>\n",
       "      <td>{'mse': 1.0583384160689933}</td>\n",
       "      <td>{'pearsonr': 0.8310107009076997}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.702100</td>\n",
       "      <td>1.122387</td>\n",
       "      <td>{'mae': 0.6964024093550474}</td>\n",
       "      <td>{'mse': 1.0588999083159847}</td>\n",
       "      <td>{'pearsonr': 0.8292977448476997}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>1.123654</td>\n",
       "      <td>{'mae': 0.6965819857447281}</td>\n",
       "      <td>{'mse': 1.0595032550311745}</td>\n",
       "      <td>{'pearsonr': 0.8291465225411373}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-500\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-500\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1000\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1000\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1500\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1500\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-1500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2000\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2000\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2000\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2500\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2500\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-2500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3000\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3000\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3000\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "Saving model checkpoint to C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3500\n",
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3500\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1/checkpoint-3500\\pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 197\n",
      "  Batch size = 16\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=11.158131356375558, metrics={'train_runtime': 393.6296, 'train_samples_per_second': 140.31, 'train_steps_per_second': 8.892, 'total_flos': 508877689743360.0, 'train_loss': 11.158131356375558, 'epoch': 70.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c152ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model\\config.json\n",
      "Model weights saved in C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#take care of distributed/paralelle training \n",
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model \n",
    "model_to_save.save_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fb37c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.109,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.144,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 464,\n",
      "  \"is_gpu\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 515,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"norm_mean\": [\n",
      "    11.199569164274653,\n",
      "    -0.9728601944583675,\n",
      "    11.199595401578872,\n",
      "    0.1914454376660732,\n",
      "    0.608589373135307,\n",
      "    365.064017672,\n",
      "    342.24912812000014,\n",
      "    364.6033136038417,\n",
      "    134.06547,\n",
      "    0.004249,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    1.1861084842221647,\n",
      "    1.890967178564785,\n",
      "    2.519587985439997,\n",
      "    2.0112818114267816,\n",
      "    795.5621221754437,\n",
      "    18.14439203724506,\n",
      "    14.536240385432393,\n",
      "    15.215140271072487,\n",
      "    12.068994414289726,\n",
      "    8.453657900068215,\n",
      "    9.114162139055054,\n",
      "    6.434168605708085,\n",
      "    7.215103879809845,\n",
      "    4.436200487997215,\n",
      "    5.109730699855831,\n",
      "    3.055231525907226,\n",
      "    3.6252747118486264,\n",
      "    -2.202564923376624,\n",
      "    18.195385007867852,\n",
      "    7.9706993589944775,\n",
      "    4.5379164631837545,\n",
      "    150.95250337667272,\n",
      "    13.184208966483704,\n",
      "    8.814008658052902,\n",
      "    3.8191839078987306,\n",
      "    3.4969386790830774,\n",
      "    2.9222201316693712,\n",
      "    2.644444123964607,\n",
      "    6.408740449956927,\n",
      "    4.95314480536345,\n",
      "    2.6263770771853108,\n",
      "    2.4113616526384853,\n",
      "    26.24052195128434,\n",
      "    37.102909834641714,\n",
      "    19.89943953042712,\n",
      "    16.353848799228413,\n",
      "    15.638332143998122,\n",
      "    21.706094849865753,\n",
      "    0.28727529762970366,\n",
      "    8.054432014422119,\n",
      "    3.2648099385428853,\n",
      "    32.629006626588726,\n",
      "    16.26551059790217,\n",
      "    47.70605007162041,\n",
      "    0.0,\n",
      "    5.325837027308287,\n",
      "    9.698460925314944,\n",
      "    5.573601891254677,\n",
      "    2.581492771453006,\n",
      "    7.3124961943884665,\n",
      "    33.07539073817076,\n",
      "    10.718462271839512,\n",
      "    6.99277406210818,\n",
      "    31.684923475431933,\n",
      "    36.92162447084414,\n",
      "    1.2074202610211657,\n",
      "    5.110701506051421,\n",
      "    0.0,\n",
      "    71.04050338999998,\n",
      "    9.57750975344203,\n",
      "    10.066085526965992,\n",
      "    0.07691213090851719,\n",
      "    13.38923196114951,\n",
      "    16.862422387837878,\n",
      "    21.382953923695233,\n",
      "    15.651918121909311,\n",
      "    14.440634953378058,\n",
      "    19.13130604146014,\n",
      "    22.114944705243296,\n",
      "    8.183429061888226,\n",
      "    13.699768012021506,\n",
      "    2.1212691930096144,\n",
      "    17.474216494453906,\n",
      "    7.8467696174922725,\n",
      "    2.6683841482907034,\n",
      "    0.11868201225906093,\n",
      "    9.064881467380093,\n",
      "    2.659801877718109,\n",
      "    4.055917032498944,\n",
      "    0.259848432909807,\n",
      "    0.413963629624058,\n",
      "    25.186704,\n",
      "    1.79722,\n",
      "    5.353545,\n",
      "    0.272499,\n",
      "    0.562898,\n",
      "    0.835397,\n",
      "    1.236854,\n",
      "    0.729917,\n",
      "    1.966771,\n",
      "    4.216321,\n",
      "    1.414081,\n",
      "    6.486208,\n",
      "    5.688314,\n",
      "    0.205632,\n",
      "    0.409204,\n",
      "    0.614836,\n",
      "    2.802168,\n",
      "    2.7549044689500004,\n",
      "    97.31541557350002,\n",
      "    0.069051,\n",
      "    0.151924,\n",
      "    0.130758,\n",
      "    0.06279,\n",
      "    0.027038,\n",
      "    0.999062,\n",
      "    0.096951,\n",
      "    0.042862,\n",
      "    0.096089,\n",
      "    0.100163,\n",
      "    1.033857,\n",
      "    1.034286,\n",
      "    0.016206,\n",
      "    0.00357,\n",
      "    0.016776,\n",
      "    1.488795,\n",
      "    0.915699,\n",
      "    0.232236,\n",
      "    0.012241,\n",
      "    0.074885,\n",
      "    0.131561,\n",
      "    0.096951,\n",
      "    0.004026,\n",
      "    0.009835,\n",
      "    0.011646,\n",
      "    0.250196,\n",
      "    0.131237,\n",
      "    0.768633,\n",
      "    0.015927,\n",
      "    0.539599,\n",
      "    0.451885,\n",
      "    0.001726,\n",
      "    0.003335,\n",
      "    0.001218,\n",
      "    1.236474,\n",
      "    0.000226,\n",
      "    0.555529,\n",
      "    0.000149,\n",
      "    0.001046,\n",
      "    0.002578,\n",
      "    0.126995,\n",
      "    0.732216,\n",
      "    0.037978,\n",
      "    0.019179,\n",
      "    0.720141,\n",
      "    0.018951,\n",
      "    0.013025,\n",
      "    0.059523,\n",
      "    0.027553,\n",
      "    0.000831,\n",
      "    0.0002,\n",
      "    0.073914,\n",
      "    0.061694,\n",
      "    0.002249,\n",
      "    0.007716,\n",
      "    0.236426,\n",
      "    0.0287,\n",
      "    0.05231,\n",
      "    0.041425,\n",
      "    0.033421,\n",
      "    0.017275,\n",
      "    0.001082,\n",
      "    0.011915,\n",
      "    0.004249,\n",
      "    0.196769,\n",
      "    0.039316,\n",
      "    0.038686,\n",
      "    0.00409,\n",
      "    0.003615,\n",
      "    0.116124,\n",
      "    0.051192,\n",
      "    0.025177,\n",
      "    0.0,\n",
      "    0.161908,\n",
      "    0.315775,\n",
      "    0.087229,\n",
      "    0.079586,\n",
      "    0.023227,\n",
      "    0.005966,\n",
      "    0.007901,\n",
      "    0.050376,\n",
      "    0.000186,\n",
      "    0.065723,\n",
      "    0.380193,\n",
      "    0.051566\n",
      "  ],\n",
      "  \"norm_std\": [\n",
      "    2.9210526350021033,\n",
      "    1.5294133532822065,\n",
      "    2.9209947673330334,\n",
      "    0.21956154740898992,\n",
      "    0.22097666681598954,\n",
      "    160.48566423804579,\n",
      "    151.38170855657367,\n",
      "    160.3304390667665,\n",
      "    60.484857692625106,\n",
      "    0.181038611279414,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.24851193112366385,\n",
      "    0.317494124851492,\n",
      "    0.37175815103599535,\n",
      "    0.6098706561111424,\n",
      "    539.8195290502504,\n",
      "    8.140940922894863,\n",
      "    6.600767667198695,\n",
      "    6.700942921964325,\n",
      "    5.536318526756788,\n",
      "    4.020569431789569,\n",
      "    4.316039675035455,\n",
      "    3.229701298304296,\n",
      "    4.058753110098356,\n",
      "    2.399274478688092,\n",
      "    4.590084765547685,\n",
      "    1.8657465201411236,\n",
      "    8.197075845395899,\n",
      "    1.3989800795766576,\n",
      "    8.727770321711972,\n",
      "    4.719034225006412,\n",
      "    3.6844834579923407,\n",
      "    66.65125255607474,\n",
      "    11.022808176926917,\n",
      "    9.88512023443511,\n",
      "    5.895101555004671,\n",
      "    6.0315631910071374,\n",
      "    4.465786134186721,\n",
      "    8.73293454096314,\n",
      "    7.292192943139112,\n",
      "    5.798809757257198,\n",
      "    5.458840154330179,\n",
      "    5.34562222799046,\n",
      "    28.624753237838462,\n",
      "    22.7685485030176,\n",
      "    13.735506972569182,\n",
      "    12.75558914023291,\n",
      "    12.647297666063738,\n",
      "    16.73803715869515,\n",
      "    1.3236865505015507,\n",
      "    8.012917117258175,\n",
      "    6.328266302270954,\n",
      "    30.80439768300023,\n",
      "    14.510669158473307,\n",
      "    33.76748799216324,\n",
      "    0.0,\n",
      "    8.851153866015428,\n",
      "    8.222102882220607,\n",
      "    7.329351085680612,\n",
      "    4.87773057457412,\n",
      "    10.796349487508557,\n",
      "    24.55359833254403,\n",
      "    10.33295824604808,\n",
      "    8.986884190324291,\n",
      "    26.77991276665104,\n",
      "    29.521288543995215,\n",
      "    4.077418430037268,\n",
      "    11.23487898363004,\n",
      "    0.0,\n",
      "    50.277243284807206,\n",
      "    19.12173183245714,\n",
      "    9.819697177666312,\n",
      "    1.4201437981599128,\n",
      "    12.511435257208836,\n",
      "    14.212538029397628,\n",
      "    16.973978925056553,\n",
      "    19.21649041911615,\n",
      "    15.092240504961104,\n",
      "    19.889237093009676,\n",
      "    25.80872442073538,\n",
      "    9.254317550453825,\n",
      "    19.013243564373347,\n",
      "    3.6841568734614953,\n",
      "    17.690679185577395,\n",
      "    10.27595457263499,\n",
      "    3.3283202642652645,\n",
      "    2.8773795244438474,\n",
      "    9.228734822190495,\n",
      "    5.106296483962912,\n",
      "    4.008127533955226,\n",
      "    2.3345092198667503,\n",
      "    0.23958883840178574,\n",
      "    11.48532061063049,\n",
      "    2.0042680181777808,\n",
      "    3.411142707197923,\n",
      "    0.7103265443180337,\n",
      "    0.8009597262862117,\n",
      "    1.0630493791282618,\n",
      "    1.2495037990913607,\n",
      "    0.8592211073826755,\n",
      "    1.4909738617970663,\n",
      "    2.8049912821495706,\n",
      "    1.5692082041123125,\n",
      "    3.7188860712382157,\n",
      "    4.918753910447648,\n",
      "    0.6213838320183964,\n",
      "    0.6971589290933399,\n",
      "    0.9385507839118636,\n",
      "    1.7370945619837506,\n",
      "    2.7759468746763334,\n",
      "    43.91556441471313,\n",
      "    0.2929625321198007,\n",
      "    0.6742399816263887,\n",
      "    0.6447563579731193,\n",
      "    0.26136083143708466,\n",
      "    0.1703202147866646,\n",
      "    1.3696411924562566,\n",
      "    0.3394696140137124,\n",
      "    0.26977939457438505,\n",
      "    0.3350074869447194,\n",
      "    0.3408584597974497,\n",
      "    1.2690580420372088,\n",
      "    1.2684116362885036,\n",
      "    0.1297126917051003,\n",
      "    0.06304965563156611,\n",
      "    0.17914965229828922,\n",
      "    1.485673805113914,\n",
      "    1.1656052934139842,\n",
      "    0.5018632205797633,\n",
      "    0.15576643470973517,\n",
      "    0.2883562378800223,\n",
      "    0.3774901929558512,\n",
      "    0.3394696140137124,\n",
      "    0.07983606764988928,\n",
      "    0.10307416455777559,\n",
      "    0.11692041889415362,\n",
      "    1.0010868912132271,\n",
      "    0.7705779932112281,\n",
      "    1.157481598590082,\n",
      "    0.13507534533122212,\n",
      "    0.8359812306885952,\n",
      "    0.7600865243553028,\n",
      "    0.04757124327808961,\n",
      "    0.07183232513905516,\n",
      "    0.03513570421263404,\n",
      "    1.239225396368063,\n",
      "    0.015097985029438593,\n",
      "    1.3364349277900949,\n",
      "    0.013378265133341392,\n",
      "    0.032663541616103894,\n",
      "    0.060970137226002974,\n",
      "    0.44400840883756576,\n",
      "    1.159532265122051,\n",
      "    0.198246590935912,\n",
      "    0.1491817288215558,\n",
      "    1.28126795861232,\n",
      "    0.143114919141507,\n",
      "    0.11579880303510387,\n",
      "    0.25012811724209466,\n",
      "    0.1830406121462275,\n",
      "    0.03504726333553974,\n",
      "    0.015295758691880374,\n",
      "    0.3034514997274073,\n",
      "    0.2749689545601939,\n",
      "    0.04859983910409953,\n",
      "    0.09878498419533764,\n",
      "    0.5707110234042025,\n",
      "    0.17028898672063034,\n",
      "    0.24456026600763192,\n",
      "    0.21322057789532142,\n",
      "    0.1917343827305721,\n",
      "    0.13591391704896466,\n",
      "    0.03519702423260403,\n",
      "    0.11080182783711219,\n",
      "    0.0680510883818226,\n",
      "    0.5264724473438641,\n",
      "    0.2602735481879015,\n",
      "    0.25847912916802446,\n",
      "    0.10886360159063149,\n",
      "    0.10026934640727359,\n",
      "    0.35113436163289397,\n",
      "    0.2260341350934195,\n",
      "    0.16874580630684471,\n",
      "    0.0,\n",
      "    0.4146998571400424,\n",
      "    0.5347143492505464,\n",
      "    0.3137422508894841,\n",
      "    0.27962501103110715,\n",
      "    0.1547563582555832,\n",
      "    0.08130444916739461,\n",
      "    0.08949068223889126,\n",
      "    0.22530492534853602,\n",
      "    0.014421012861987593,\n",
      "    0.2736413019822887,\n",
      "    2.253629375384596,\n",
      "    0.22817317920167496\n",
      "  ],\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 600\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at C:/Users/pj11/Documents/bert_finetune2/hansen_d_fold1//model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: smiles. If smiles are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 197\n",
      "  Batch size = 32\n",
      "C:\\Users\\pj11\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making prediction \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_output_path)\n",
    "\n",
    "# arguments for Trainer\n",
    "test_args = TrainingArguments(\n",
    "     output_dir =model_output_path,\n",
    "     do_train = False,\n",
    "     do_predict = True,\n",
    "     dataloader_drop_last = False\n",
    ")\n",
    "\n",
    "# Init Trainer\n",
    "trainer=Trainer(\n",
    "          model = model,\n",
    "          args = test_args,\n",
    "          compute_metrics = compute_metrics)\n",
    "\n",
    "test_results = trainer.predict(small_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27ef5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.8, 16. , 15.6, 15.7, 17.8, 15.9, 17.2, 17.2, 18.8, 16. , 17.8,\n",
       "       16.5, 15. , 20. , 20. , 15.2, 15.4, 19.9, 20.2, 14. , 16. , 21. ,\n",
       "       14.4, 16.8, 17. , 19.4, 15.9, 15.8, 19.6, 19.5, 17.8, 18.7, 19. ,\n",
       "       19.2, 16.3, 20.3, 16.5, 17.1, 15.8, 18.3, 15.4, 15.4, 20.9, 15.3,\n",
       "       15.7, 15. , 17.5, 14.7, 15.8, 16. , 19.9, 19. , 15.5, 19.8, 18.5,\n",
       "       20.3, 16.5, 15.7, 15.7, 15.4, 15.8, 16.8, 16.2, 19. , 16.8, 16.4,\n",
       "       14.5, 18.7, 18.7, 19. , 16.3, 15.1, 16.8, 20.2, 14.9, 18.2, 20.5,\n",
       "       18.2, 14.6, 19.1, 15. , 16.4, 15.5, 20.4, 20.1, 15. , 16. , 15.9,\n",
       "       16.7, 20. , 15.5, 19.2, 15.6, 19.5, 17.3, 16.3, 16.4, 20.2, 17.5,\n",
       "       17.2, 16.1, 19.8, 18.7, 20. , 19. , 19.4, 15.3, 16.5, 19.5, 16.2,\n",
       "       20. , 16.9, 15.6, 17.4, 19. , 20. , 20. , 15.8, 19.5, 15.6, 17.5,\n",
       "       13.7, 19. , 19.2, 17.8, 16. , 16.3, 14.8, 16. , 15.3, 16.1, 17.8,\n",
       "       17.4, 21.3, 20.2, 18.9, 18.9, 20.7, 14.4, 20. , 17.6, 12.2, 14.6,\n",
       "       15.2, 16.8, 16.4, 16. , 18.2, 16. , 18.1, 17.4, 15. , 19.9, 19.1,\n",
       "       18.3, 14.9, 16.5, 18.6, 18. , 19.7, 12.8, 15.7, 19.7, 15.8, 16.6,\n",
       "       14.6, 17.1, 17.5, 17.4, 18.6, 15.9, 18.1, 20. , 16. , 17.8, 12.3,\n",
       "       18.3, 17.8, 18.9, 15.3, 15.7, 15.2, 21. , 17.4, 19.5, 17.4, 15.3,\n",
       "       17.9, 16.5, 17.6, 16.2, 15.5, 16.4, 15.8, 15.2, 18.8, 15. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out prediction in test set\n",
    "test_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09656a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.827226996421814,\n",
       " 'test_mae': {'mae': 0.6462542877584545},\n",
       " 'test_rmse': {'mse': 0.909957019184815},\n",
       " 'test_pearsonr': {'pearsonr': 0.880799222914437},\n",
       " 'test_runtime': 1.045,\n",
       " 'test_samples_per_second': 188.523,\n",
       " 'test_steps_per_second': 6.699}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out metric in test set\n",
    "test_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "362ec747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64625436\n",
      "0.9099570453377971\n",
      "0.6845268369984588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "\n",
    "# MAE, AE and RMSE give an idea of the error distribution\n",
    "print(mean_absolute_error(test_results[0], test_results[1]))\n",
    "\n",
    "#RMSEs\n",
    "print(math.sqrt(mean_squared_error(test_results[0], test_results[1])))\n",
    "\n",
    "# R^2 Coefficient of Determination\n",
    "print(r2_score(test_results[0], test_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef967db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred. Hansen d')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot exp vs pred in test set\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "\n",
    "ln = np.arange(10, 25, 0.2)\n",
    "plt.plot(ln, ln,'r--')\n",
    "plt.scatter(test_results[1], test_results[0])\n",
    "plt.xlabel('exp. Hansen d')\n",
    "plt.ylabel('pred. Hansen d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9c240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_pred_test=pd.DataFrame(test_results[0], columns=[\"predict\"])\n",
    "pd_exp_test=pd.DataFrame(test_results[1], columns=[\"exp\"])\n",
    "pd_smiles=pd.DataFrame(dataset['test']['smiles'], columns=[\"smiles\"])\n",
    "pd_test=pd.concat((pd_smiles, pd_exp_test, pd_pred_test), axis=1)\n",
    "\n",
    "# save predicton to csv \n",
    "pd_test.to_csv('hansen_d_bert_ds6_fold1_results_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5108c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04564f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a5612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
